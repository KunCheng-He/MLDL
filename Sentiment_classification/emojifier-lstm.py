from torch import nn, optim
from emo_utils import read_csv, read_glove_vecs, label_to_one_hot, label_to_emoji
import torch
import visdom


# ä¸€ä¸ªå¥å­è½¬ä¸º tensor
def an_sentence_to_tensor(sentence, word_to_vec_map):
    sentence = sentence.lower().split()
    vec = word_to_vec_map[sentence[0]].reshape(1, -1)
    for i in sentence[1:]:
        vec = torch.cat((vec, word_to_vec_map[i].reshape(1, -1)), dim=0)
    return vec.unsqueeze(dim=1)


# æ‰€æœ‰çš„æ–‡æœ¬è½¬ä¸º tensorï¼Œå› ä¸ºæ¯ä¸ªå¥å­ä¸ä¸€æ ·é•¿ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ç”¨åˆ—è¡¨è¿›è¡Œå­˜å‚¨
def all_text_to_tensor(text, word_to_vec_map):
    tensor_list = []
    for i in text:
        tensor_list.append(an_sentence_to_tensor(i, word_to_vec_map))
    return tensor_list


# è®¡ç®—æ¨¡å‹æ­£ç¡®ç‡
def cal_acc(net, x, y):
    acc_num = 0
    for i, j in zip(x, y):
        y_hat = net(i).argmax(dim=1).item()
        if y_hat == j.item():
            acc_num += 1
    return acc_num / len(x)


# å®šä¹‰ç½‘ç»œ
class Lstm(nn.Module):
    def __init__(self) -> None:
        super(Lstm, self).__init__()
        self.lstm = nn.LSTM(50, 30)
        self.line = nn.Linear(30, 5)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        out, (ht, ct) = self.lstm(x)  # é»˜è®¤çš„ä¸¤ä¸ªå‚æ•°ä¸º 0
        out = out.reshape(out.shape[0], 30)
        out = out.sum(dim=0, keepdim=True)
        out = self.softmax(self.line(out))
        return out


if __name__ == "__main__":
    # åŠ è½½æ•°æ®é›†
    x_train, y_train = read_csv("./emo_data/train_emoji.csv")
    x_test, y_test = read_csv("./emo_data/tesss.csv")

    # åŠ è½½é¢„è®­ç»ƒçš„ 50ç»´GloVe æ¨¡å‹
    word_to_vec_map = read_glove_vecs("emo_data/glove.6B.50d.txt")

    # å°†è¾“å…¥å’Œæ ‡ç­¾è½¬æ¢ä¸ºæ‰€éœ€è¦å½¢çŠ¶çš„ å¼ é‡
    x_train = all_text_to_tensor(x_train, word_to_vec_map)
    y_train = torch.from_numpy(label_to_one_hot(y_train, 5))
    x_test = all_text_to_tensor(x_test, word_to_vec_map)
    y_test = torch.from_numpy(y_test)

    # å¯è§†åŒ–ä¸€äº›å‚æ•°
    vis = visdom.Visdom()
    vis.line([0.], [0.], win="train_loss", opts=dict(title="train loss"))  # ç›‘å¬è®­ç»ƒæŸå¤±
    vis.line([0.], [0.], win="test_acc", opts=dict(title="test acc"))

    # å¼€å§‹è®­ç»ƒ
    epochs = 60
    lr = 0.01
    net = Lstm()
    loss = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters(), lr=lr)
    print("train start...")
    for epoch in range(epochs):
        l = 0.0
        net.train()
        for x, y in zip(x_train, y_train):
            y_hat = net(x)  # x.shape: (seq, batch=1, feature=50)
            ls = loss(y_hat, y.reshape(1, -1))
            l += ls.item()

            optimizer.zero_grad()
            ls.backward()
            optimizer.step()
        
        net.eval()
        acc = cal_acc(net, x_test, y_test)
        vis.line([l], [epoch], win="train_loss", update="append")
        vis.line([acc], [epoch], win="test_acc", update="append")
        # print("epoch {} ---> loss: {:5f} ---> acc: {:5f}".format(epoch+1, l, acc))
    print("train over...")

    tips = """
    -----------------------------------------------------------------
    ä»¥ä¸‹æ˜¯ä¸€ä¸ªæµ‹è¯•ç¨‹åºï¼Œä½ å¯ä»¥è‡ªå·±è¾“å…¥ä¸€å¥è‹±æ–‡å¥å­ï¼Œæ¨¡å‹å¯ä»¥ç»™å‡ºç›¸åº”çš„ emoji è¡¨æƒ…
    ä¾‹å¥ï¼šI want a big meal ---> ğŸ´
         You love me ---> â¤ï¸ 
         You very happy ---> ğŸ˜€
    è¾“å…¥ 0 é€€å‡º
    -----------------------------------------------------------------
    """
    print(tips)
    sentence = input("è¯·è¾“å…¥ä¸€ä¸ªè‹±æ–‡å¥å­ï¼š")
    while True:
        if sentence == '0':
            break
        x = an_sentence_to_tensor(sentence, word_to_vec_map)
        y = net(x).argmax(dim=1).item()
        print("ä½ å¯ä»¥æ­é…ï¼š", label_to_emoji(y))
        sentence = input("è¯·è¾“å…¥ä¸€ä¸ªè‹±æ–‡å¥å­ï¼š")
